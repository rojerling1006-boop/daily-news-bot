import requests
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator
import time 

# --- è¨­å®šå€ (ç²¾é¸ 16 å€‹æ–°èä¾†æº) ---
news_sources = [
    # === åŸæœ¬ä¿ç•™çš„ 7 å€‹ ===
    { "name": "AP News (ç¾è¯ç¤¾)", "url": "https://apnews.com/hub/world-news", "tag": "h3", "root": "https://apnews.com" },
    { "name": "CNN", "url": "https://edition.cnn.com/world", "tag": "span", "root": "https://edition.cnn.com" },
    { "name": "BBC News", "url": "https://www.bbc.com/news", "tag": "h2", "root": "https://www.bbc.com" },
    { "name": "The Guardian (è¡›å ±)", "url": "https://www.theguardian.com/international", "tag": "h3", "root": "" },
    { "name": "NPR (ç¾åœ‹å…¬å…±å»£æ’­)", "url": "https://www.npr.org/sections/news/", "tag": "h2", "root": "" },
    { "name": "Al Jazeera (åŠå³¶é›»è¦–å°)", "url": "https://www.aljazeera.com/news/", "tag": "h3", "root": "https://www.aljazeera.com" },
    { "name": "Nature (ç§‘å­¸æœŸåˆŠ)", "url": "https://www.nature.com/news", "tag": "h3", "root": "" },
    
    # === æ–°å¢çš„ (å·²ç§»é™¤é«˜é˜²ç¦¦ç¶²ç«™) ===
    { "name": "The New York Times (ç´ç´„æ™‚å ±)", "url": "https://www.nytimes.com/section/world", "tag": "h3", "root": "https://www.nytimes.com" },
    { "name": "The Washington Post (è¯ç››é “éƒµå ±)", "url": "https://www.washingtonpost.com/world", "tag": "h2", "root": "" },
    { "name": "Nikkei Asia (æ—¥ç¶“)", "url": "https://asia.nikkei.com/", "tag": "h4", "root": "https://asia.nikkei.com" }, 
    { "name": "Le Monde (ä¸–ç•Œå ±)", "url": "https://www.lemonde.fr/en/", "tag": "h3", "root": "" },
    { "name": "Der Spiegel (æ˜é¡å‘¨åˆŠ)", "url": "https://www.spiegel.de/international/", "tag": "h3", "root": "" },
    { "name": "Deutsche Welle (å¾·åœ‹ä¹‹è²)", "url": "https://www.dw.com/en/top-stories/s-9097", "tag": "h3", "root": "https://www.dw.com" },
    { "name": "El PaÃ­s (åœ‹å®¶å ±)", "url": "https://english.elpais.com/", "tag": "h2", "root": "https://english.elpais.com" },
    { "name": "Xinhua (æ–°è¯ç¤¾)", "url": "https://english.news.cn/", "tag": "span", "root": "" },
    { "name": "SCMP (å—è¯æ—©å ±)", "url": "https://www.scmp.com/news/world", "tag": "h2", "root": "https://www.scmp.com" }
]

# åˆå§‹åŒ–ç¿»è­¯å™¨
translator = GoogleTranslator(source='auto', target='zh-TW')

# --- å½è£é ­ ---
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9",
    "Referer": "https://www.google.com/"
}

# --- æ­¥é©Ÿ 1: æ¸…ç©ºèˆŠæª” ---
print("ğŸ§¹ æ­£åœ¨æ¸…ç©ºèˆŠçš„æ–°èæª”æ¡ˆ...")
with open("news_report.txt", "w", encoding="utf-8") as file:
    file.write("=== æ¯æ—¥é‡é»æ–°èå½™æ•´ (16 Sources) ===\n\n")

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•¸ ---
def get_news(source_config):
    url = source_config["url"]
    tag = source_config["tag"]
    root_url = source_config["root"]
    site_name = source_config["name"]

    print(f"ğŸš€ æ­£åœ¨å‰å¾€ {site_name}...")
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            items = []

            # --- ç¶²ç«™å°ˆå±¬è™•ç†é‚è¼¯ ---
            if site_name == "CNN":
                items = soup.find_all("span", class_="container__headline-text")
                if not items: items = soup.find_all("span")
            
            elif site_name == "Xinhua (æ–°è¯ç¤¾)":
                items = soup.find_all("div", class_="tit") 
                if not items: items = soup.find_all("span")

            elif site_name == "Nikkei Asia (æ—¥ç¶“)":
                 items = soup.find_all("h4")
            
            else:
                items = soup.find_all(tag)
            # -----------------------

            if len(items) > 0:
                print(f"   âœ… æˆåŠŸé€£ç·šï¼Œæ‰¾åˆ° {len(items)} å€‹æ½›åœ¨æ¨™é¡Œ")
            else:
                print(f"   âš ï¸ é€£ç·šæˆåŠŸä½†æ‰¾ä¸åˆ°æ¨™é¡Œ")

            count = 0
            seen_titles = set()

            for item in items:
                if count >= 5: break
                
                # æŠ“å–é€£çµ
                if tag in ["h2", "h3", "h4", "span", "div"]:
                    link = item.find_parent("a")
                    if not link: link = item.find("a")
                else:
                    link = item 
                
                # æŠ“å–æ¨™é¡Œæ–‡å­—
                headline_en = ""
                if item.get_text(strip=True):
                    headline_en = item.get_text(strip=True)
                elif link and link.get_text(strip=True):
                    headline_en = link.get_text(strip=True)

                if headline_en and len(headline_en) > 10 and headline_en not in seen_titles:
                    seen_titles.add(headline_en)
                    
                    try:
                        headline_zh = translator.translate(headline_en)
                        print(f"   ğŸ“° {headline_zh}")
                    except:
                        headline_zh = headline_en
                        print(f"   ğŸ“° {headline_zh} (ç¿»è­¯ç•¥é)")
                    
                    if link:
                        link_url = link.get("href")
                        if link_url:
                            if not link_url.startswith("http"):
                                link_url = root_url + link_url
                            
                            with open("news_report.txt", "a", encoding="utf-8") as file:
                                file.write(f"ã€{site_name}ã€‘{headline_zh}\n{link_url}\n\n")
                            
                            count += 1
            
            if count == 0 and response.status_code == 200:
                print("   âš ï¸ æ²’æŠ“åˆ°ç¬¦åˆæ¢ä»¶çš„æ–°èã€‚")

        else:
            if response.status_code in [401, 403]:
                print(f"   ğŸš« è¢«é˜»æ“‹ (Error {response.status_code}): è©²ç¶²ç«™æœ‰åš´æ ¼é˜²çˆ¬èŸ²æ©Ÿåˆ¶")
            else:
                print(f"   âŒ é€£ç·šå¤±æ•—: {response.status_code}")

    except Exception as e:
        print(f"   âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")

    print("-" * 30)
    time.sleep(1)

# --- ä¸»ç¨‹å¼å€ (ä¸éœ€è¦ while loopï¼Œä¹Ÿä¸éœ€è¦ job å‡½æ•¸åŒ…è£) ---
print(f"â° é–‹å§‹åŸ·è¡Œæ¯æ—¥æ–°èæŠ“å–... ç¾åœ¨æ™‚é–“: {time.strftime('%H:%M:%S')}")

for source in news_sources:
    get_news(source)

print("ğŸ’¤ ä»»å‹™å®Œæˆï¼")
